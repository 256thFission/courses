---
title: "Modular Evaluation Analysis: STA Department Demo"
author: "Statistical Evaluation Pipeline"
format: 
  html:
    code-fold: false
    toc: true
    theme: cosmo
execute:
  warning: false
  message: false
---

# Introduction

COMPARE NLP OF COURSE EVALS OF MALE VS FEMALE PROFS

## Setup

```{r setup}
# Load required libraries
library(dplyr)
library(binom)
library(stringr)
library(knitr)
library(ggplot2)
library(polycor)
library(corrplot)
library(psych)

# Source our modular pipeline
source("evaluation_pipeline.R")

# Load the STA evaluation data
sta_data <- read.csv("data/STA_evaluations.csv")

# Quick look at the data structure
cat("Dataset dimensions:", nrow(sta_data), "rows,", ncol(sta_data), "columns\n")
cat("Unique instructors:", length(unique(sta_data$instructor)), "\n")
cat("Date range:", paste(range(sta_data$semester), collapse = " to "), "\n")
```

```{r}
head(sta_data,10)
```

## Available Questions for Analysis

```{r explore-questions}
# Discover what ordinal questions are available in our dataset
available_questions <- get_available_questions(sta_data)
cat("Available ordinal questions for analysis:\n")
for(i in seq_along(available_questions)) {
  cat(sprintf("%d. %s\n", i, available_questions[i]))
}
```

# Analysis 1: Instructor Performance Analysis

Let's analyze instructor performance using the default question (overall instructor rating).

```{r instructor-analysis}
# Analyze instructors using the main pipeline function
instructor_results <- analyze_instructors(sta_data)

# Print summary
print_evaluation_summary(instructor_results)
```

## Instructor Results Details

```{r instructor-details}
# Show top performers in high confidence tier
high_conf_instructors <- instructor_results$data %>%
  filter(confidence_tier == "High Confidence") %>%
  arrange(desc(percent_top_n)) %>%
  select(instructor, percent_top_n, ci_lower_pct, ci_upper_pct, totalStudents, numClasses)

cat("High Confidence Instructors (Top 2 Rating %):\n")
kable(high_conf_instructors, digits = 1, 
      col.names = c("Instructor", "Top 2 %", "CI Lower", "CI Upper", "Total Responses", "Classes"))
```

## Visualization: Instructor Performance by Confidence Tier

```{r instructor-plot}
# Create a visualization of instructor performance by tier
ggplot(instructor_results$data, 
       aes(x = totalStudents, y = percent_top_n, color = confidence_tier)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_errorbar(aes(ymin = ci_lower_pct, ymax = ci_upper_pct), 
                width = 0, alpha = 0.5) +
  scale_color_manual(values = c(
    "High Confidence" = "#2E8B57",
    "Moderate Confidence" = "#FFB347", 
    "Low Confidence" = "#FFB6C1",
    "Preliminary Only" = "#D3D3D3"
  )) +
  labs(
    title = "STA Instructor Performance: Top 2 Rating Percentage",
    subtitle = "With Wilson 95% Confidence Intervals",
    x = "Total Student Responses",
    y = "Percentage Rating 4 or 5 (Top 2)",
    color = "Confidence Tier"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

# Analysis 2: Course Quality by Subject Level

Let's analyze course quality (Q5) grouped by course level (extracted from course titles).

```{r course-level-analysis}
# First, let's extract course level from course titles
sta_data_with_level <- sta_data %>%
  mutate(
    course_level = case_when(
      str_detect(course_title, "-(1|2)\\d\\d") ~ "Undergraduate",
      str_detect(course_title, "-(3|4)\\d\\d") ~ "Advanced Undergraduate", 
      str_detect(course_title, "-(5|6|7|8|9)\\d\\d") ~ "Graduate",
      TRUE ~ "Other"
    )
  )

# Analyze course quality by level
level_analysis <- analyze_evaluations(
  sta_data_with_level,
  group_var = "course_level",
  question_stem = "Q5_overall_course_quality",
  top_n = 2
)

print_evaluation_summary(level_analysis)
```

# Analysis 3: Workload Analysis with Different Parameters

Let's analyze course difficulty (Q8) with different parameters - top 3 levels and fixed tiers.

```{r difficulty-analysis}
# Analyze course difficulty using top 3 levels and fixed sample size tiers
difficulty_analysis <- analyze_evaluations(
  sta_data,
  group_var = "instructor", 
  question_stem = "Q8_course_difficulty",
  top_n = 3,  # Top 3 levels (moderate to very difficult)
  tier_method = "fixed",  # Use fixed thresholds instead of quartiles
  conf_level = 0.99  # 99% confidence intervals
)

print_evaluation_summary(difficulty_analysis)
```

# Analysis 4: Polychoric Correlation Analysis

Now let's explore relationships between ordinal questions using polychoric correlations, which are appropriate for Likert scale data.

## Overall Question Correlations

```{r correlation-analysis}
# Calculate polychoric correlations between all questions
sta_correlations <- analyze_correlations(
  sta_data,
  min_responses = 30  # Minimum responses to include a question
)

# Print summary
print_correlation_summary(sta_correlations)
```

## Visualization of Correlations

```{r correlation-plot, fig.width=10, fig.height=8}
# Create correlation plot
plot_polychoric_correlations(
  sta_correlations,
  method = "circle",
  title = "STA Department: Question Correlations"
)
```

## Focused Analysis: Teaching Effectiveness Questions

```{r teaching-correlations}
# Focus on teaching-related questions
teaching_questions <- c(
  "Q6_overall_instructor_rating",
  "Q9_clear_explanations", 
  "Q10_helpful_feedback"
)

# Check which questions are available
available_teaching <- intersect(teaching_questions, get_available_questions(sta_data))
cat("Available teaching questions:", paste(available_teaching, collapse = ", "), "\n")

if (length(available_teaching) >= 2) {
  teaching_corr <- analyze_correlations(
    sta_data,
    question_stems = available_teaching,
    min_responses = 25
  )
  
  # Get significant correlations
  sig_teaching <- summarize_significant_correlations(teaching_corr, alpha = 0.05)
  
  cat("\nSignificant correlations among teaching questions:\n")
  kable(sig_teaching[sig_teaching$Significant, 
                    c("Question1", "Question2", "Correlation", "P_Value")],
        digits = 3)
}
```

## Instructor-Level Correlation Analysis

```{r instructor-correlations}
# Analyze correlations at instructor level
instructor_corr <- analyze_instructor_correlations(
  sta_data,
  min_responses = 15  # Lower threshold for instructor-level analysis
)

if (!is.null(instructor_corr)) {
  cat("Instructor-level correlation analysis completed\n")
  cat("Questions analyzed:", length(instructor_corr$question_stems), "\n")
  
  # Show strongest correlations
  strong_corr <- summarize_significant_correlations(instructor_corr)
  top_corr <- strong_corr[order(abs(strong_corr$Correlation), decreasing = TRUE), ][1:5, ]
  
  cat("\nStrongest correlations at instructor level:\n")
  kable(top_corr[c("Question1", "Question2", "Correlation", "P_Value")], digits = 3)
}
```

## Integrated Analysis with Correlations

```{r integrated-with-correlations}
# Combine traditional analysis with correlation analysis
comprehensive_analysis <- analyze_evaluations(
  sta_data,
  group_var = "instructor",
  question_stem = "Q6_overall_instructor_rating", 
  include_correlations = TRUE,
  correlation_questions = get_available_questions(sta_data)[1:4],  # First 4 questions
  min_responses = 20
)

# Show traditional results
cat("=== TRADITIONAL ANALYSIS RESULTS ===\n")
print_evaluation_summary(comprehensive_analysis)

# Show correlation results if available
if (!is.null(comprehensive_analysis$correlations)) {
  cat("\n=== CORRELATION ANALYSIS RESULTS ===\n")
  print_correlation_summary(comprehensive_analysis$correlations)
}
```

## Comparison: Easy vs Difficult Instructors

```{r difficulty-comparison}
# Show instructors with highest and lowest difficulty ratings
difficulty_summary <- difficulty_analysis$data %>%
  filter(confidence_tier %in% c("High Confidence", "Moderate Confidence")) %>%
  select(instructor, percent_top_n, ci_lower_pct, ci_upper_pct, totalStudents) %>%
  arrange(desc(percent_top_n))

cat("Most Difficult Courses (Top 10):\n")
kable(head(difficulty_summary, 10), digits = 1,
      col.names = c("Instructor", "Difficult %", "CI Lower", "CI Upper", "Responses"))

cat("\nLeast Difficult Courses (Bottom 10):\n") 
kable(tail(difficulty_summary, 10), digits = 1,
      col.names = c("Instructor", "Difficult %", "CI Lower", "CI Upper", "Responses"))
```

# Analysis 4: Semester Trends

Let's analyze how ratings vary by semester.

```{r semester-analysis}
# Analyze instructor ratings by semester
semester_analysis <- analyze_evaluations(
  sta_data,
  group_var = "semester",
  question_stem = "Q6_overall_instructor_rating"
)

print_evaluation_summary(semester_analysis)

# Create a trend plot
semester_plot_data <- semester_analysis$data %>%
  arrange(semester)

ggplot(semester_plot_data, 
       aes(x = semester, y = percent_top_n, fill = confidence_tier)) +
  geom_col(alpha = 0.8) +
  geom_errorbar(aes(ymin = ci_lower_pct, ymax = ci_upper_pct), 
                width = 0.2, color = "black") +
  scale_fill_manual(values = c(
    "High Confidence" = "#2E8B57",
    "Moderate Confidence" = "#FFB347",
    "Low Confidence" = "#FFB6C1"
  )) +
  labs(
    title = "STA Department: Instructor Ratings by Semester",
    subtitle = "Percentage of Top 2 Ratings (4-5 on 5-point scale)",
    x = "Semester",
    y = "Percentage Top 2 Ratings",
    fill = "Confidence Tier"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )
```

# Analysis 5: Modular Usage Example

This demonstrates how to use individual pipeline modules for custom analysis workflows.

```{r modular-example}
cat("=== MODULAR PIPELINE DEMO ===\n\n")

# Step 1: Aggregate data by instructor
cat("Step 1: Aggregating evaluation data by instructor...\n")
aggregated <- aggregate_evaluations(sta_data, "instructor")
cat("Result: Data aggregated to", nrow(aggregated), "instructors\n\n")

# Step 2: Calculate proportions for Q3 (intellectually stimulating)
cat("Step 2: Calculating top 2 proportions for Q3...\n")
with_props <- calculate_top_n_proportion(aggregated, "Q3_intellectually_stimulating", top_n = 2)
cat("Result: Proportions calculated\n\n")

# Step 3: Add Wilson confidence intervals
cat("Step 3: Adding Wilson confidence intervals...\n")
with_ci <- add_wilson_ci(with_props, "top_n_count", "total_responses", conf_level = 0.95)
cat("Result: Confidence intervals added\n\n")

# Step 4: Create sample size tiers
cat("Step 4: Creating sample size tiers...\n")
final_data <- create_sample_tiers(with_ci, "totalStudents", method = "quartile")
cat("Result: Tiers assigned\n\n")

# Show top performers for this custom analysis
top_stimulating <- final_data %>%
  filter(confidence_tier == "High Confidence") %>%
  arrange(desc(percent_top_n)) %>%
  select(instructor, percent_top_n, ci_lower_pct, ci_upper_pct, totalStudents) %>%
  slice(1:5)

cat("Top 5 Most Intellectually Stimulating Instructors:\n")
kable(top_stimulating, digits = 1,
      col.names = c("Instructor", "Stimulating %", "CI Lower", "CI Upper", "Responses"))
```

# Summary and Key Features

## Pipeline Benefits Demonstrated

1.  **Flexible Grouping**: We analyzed by instructor, course level, and semester
2.  **Multiple Questions**: Examined instructor rating, course quality, and difficulty
3.  **Configurable Parameters**: Used different top N values, confidence levels, and tier methods
4.  **Modular Design**: Showed how individual functions can be combined for custom workflows
5.  **Rigorous Statistics**: Wilson confidence intervals provide proper uncertainty quantification
6.  **Polychoric Correlations**: Proper analysis of relationships between ordinal variables

## Sample Size Handling

The pipeline automatically creates confidence tiers based on sample sizes:

-   **High Confidence**: Top 25% of sample sizes (most reliable)
-   **Moderate Confidence**: Middle 50% of sample sizes\
-   **Low Confidence**: Bottom quartile but still analyzable
-   **Preliminary Only**: Very small samples (n â‰¥ 10) flagged as preliminary

## Next Steps

This modular pipeline can be easily extended to:

-   Add new grouping variables (department, course type, etc.)
-   Include additional questions from the evaluation\
-   Implement different statistical methods
-   Create custom visualizations and reports
-   Compare across multiple datasets
-   Explore factor structure through correlation patterns
-   Identify redundant questions through high correlations

## Statistical Rigor

The pipeline now provides comprehensive statistical analysis:

-   **Wilson Confidence Intervals**: Proper uncertainty quantification for proportions
-   **Polychoric Correlations**: Unbiased correlation estimates for ordinal data
-   **Significance Testing**: Proper p-values and confidence intervals for correlations
-   **Sample Size Considerations**: Automatic filtering based on minimum response thresholds

This combination ensures that analysis results are both statistically sound and practically interpretable, addressing the specific challenges of ordinal evaluation data.
